{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "import pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load the raw data from the data lake "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    df_orders = spark.read.csv('abfss://<container>@<account-name>.dfs.core.windows.net/<path>/orders.csv', header='true', inferSchema='true')\n",
        "    return df_orders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def remove_duplicate_orders(df):\n",
        "    #return df.distinct()\n",
        "    return df.dropDuplicates([\"OrderId\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculate reporting metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def calculate_sales_by_region(df):\n",
        "    return df \\\n",
        "        .select(\"Region\", \"TotalPrice\") \\\n",
        "        .groupBy(\"Region\") \\\n",
        "        .sum(\"TotalPrice\") \\\n",
        "        .withColumnRenamed(\"sum(TotalPrice)\", \"TotalSales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def save_output(df):\n",
        "    df.repartition(1) \\\n",
        "        .write.mode(\"overwrite\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv('abfss://<container>@<account-name>.dfs.core.windows.net/<path>/output/')\n",
        "\n",
        "    df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "orders_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def test_orders_with_duplicated_order_id_are_removed():\n",
        "    \n",
        "    # Arrange\n",
        "    df = spark.createDataFrame(\n",
        "            [\n",
        "                (10,\"01/01/2020\",\"North\",\"Chicago\",\"Bars\",\"Carrot\",33,1.77,58.41),\n",
        "                (10,\"11/03/2020\",\"North\",\"Chicago\",\"Bars\",\"Carrot\",33,1.77,58.41),\n",
        "            ],\n",
        "            orders_schema \n",
        "        )\n",
        "\n",
        "    #Act\n",
        "    df_result = remove_duplicate_orders(df)\n",
        "\n",
        "    #Assert\n",
        "    assert df_result, \"No data frame returned from remove_duplicate_orders()\"\n",
        "\n",
        "    expected_orders = 1\n",
        "    number_of_orders = df_result.count()\n",
        "    assert number_of_orders == 1, f'Expected {expected_orders} order after remove_duplicate_orders() but {number_of_orders} returned.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def test_similar_orders_with_different_order_id_are_not_removed():\n",
        "    \n",
        "    # Arrange\n",
        "    df = spark.createDataFrame(\n",
        "            [\n",
        "                (10,\"01/01/2020\",\"North\",\"Chicago\",\"Bars\",\"Carrot\",33,1.77,58.41),\n",
        "                (11,\"01/01/2020\",\"North\",\"Chicago\",\"Bars\",\"Carrot\",33,1.77,58.41),\n",
        "                (12,\"01/01/2020\",\"North\",\"Chicago\",\"Bars\",\"Carrot\",33,1.77,58.41),\n",
        "            ],\n",
        "            orders_schema \n",
        "        )\n",
        "\n",
        "    #Act\n",
        "    df_result = remove_duplicate_orders(df)\n",
        "\n",
        "    #Assert\n",
        "    expected_orders = 3\n",
        "    number_of_orders = df_result.count()\n",
        "    assert number_of_orders == 3, f'Expected {expected_orders} order after remove_duplicate_orders() but {number_of_orders} returned.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def test_regional_sales_are_calculated_correctly():\n",
        "\n",
        "    # Arrange\n",
        "    df = spark.createDataFrame(\n",
        "            [\n",
        "                (7,\"19/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",149,3.49,520.01),\n",
        "                (8,\"22/01/2020\",\"West\",\"Los Angeles\",\"Bars\",\"Carrot\",51,1.77,90.27),\n",
        "                (9,\"25/01/2020\",\"East\",\"New York\",\"Bars\",\"Carrot\",100,1.77,177.00),\n",
        "                (10,\"28/01/2020\",\"East\",\"New York\",\"Snacks\",\"Potato Chips\",28,1.35,37.8),\n",
        "            ],\n",
        "            orders_schema \n",
        "        )\n",
        "        \n",
        "    #Act\n",
        "    df_result = calculate_sales_by_region(df)\n",
        "\n",
        "    #Assert\n",
        "    expected_sales_east = 734.81\n",
        "    sales_east = df_result.where(df_result[\"Region\"] == \"East\").collect()[0][\"TotalSales\"]\n",
        "\n",
        "    assert expected_sales_east == sales_east, f'Expected regional sales to be {expected_sales_east} for East region but {sales_east} returned.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run the workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Tests\n",
        "test_orders_with_duplicated_order_id_are_removed()\n",
        "test_similar_orders_with_different_order_id_are_not_removed()\n",
        "test_regional_sales_are_calculated_correctly()\n",
        "\n",
        "#ETL\n",
        "df_orders = load_data()\n",
        "df_unique_orders = remove_duplicate_orders(df_orders)\n",
        "df_sales_by_region = calculate_sales_by_region(df_unique_orders)\n",
        "save_output(df_sales_by_region)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
